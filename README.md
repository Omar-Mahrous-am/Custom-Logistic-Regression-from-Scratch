# Logistic Regression from Scratch

This project implements **Logistic Regression** from scratch using only NumPy. It includes the full training loop with gradient descent, a sigmoid function, thresholded prediction, and accuracy evaluation using a synthetic binary classification dataset.

---

## ğŸ“Œ Features

- âœ… Pure Python (only NumPy used for model logic)
- âœ… Binary classification using sigmoid activation
- âœ… Manual gradient descent implementation
- âœ… Threshold customization for prediction
- âœ… Accuracy evaluation with scikit-learn

---

## ğŸ§  Algorithm Overview

Logistic Regression is a linear model used for binary classification. It predicts the probability that an input belongs to a certain class using the **sigmoid function**. This implementation includes:

- **Sigmoid activation** for probabilities  
- **Binary cross-entropy loss (implicitly minimized)**
- **Gradient descent** to optimize weights
- **Prediction threshold** for classification (default: 0.5)

---

## ğŸ§ª Dataset

We use a **synthetic dataset** generated with `sklearn.datasets.make_classification()` for demonstration. It has:

- 500 samples
- 5 features
- 2 target classes

---

## ğŸ› ï¸ Installation

You only need NumPy and scikit-learn:

```bash
pip install numpy scikit-learn
